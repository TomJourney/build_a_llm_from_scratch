from pathlib import Path

import tiktoken

from src.chapter06.test0603_p160_spam_dataset_module import DiySpamDataset

# 获取tiktoken中的gpt2分词器
gpt2_tokenizer = tiktoken.get_encoding("gpt2")
# 把<|endoftext|>作为填充词元，词元<|endoftext|>的词元id等于50256
print(gpt2_tokenizer.encode("<|endoftext|>", allowed_special={"<|endoftext|>"}))
# [50256]

# 创建训练集
train_dataset = DiySpamDataset(
    csv_file=Path("dataset") / "train.csv",
    max_length=None,
    tokenizer=gpt2_tokenizer
)
# 最长序列长度
print("train_dataset.max_length = ", train_dataset.max_length)
# train_dataset.max_length =  120
print(f"data[0] = ", train_dataset.__getitem__(0))
print(f"data[1] = ", train_dataset.__getitem__(1))
print(f"data[2] = ", train_dataset.__getitem__(2))
# data[0] =  (tensor([   35,  2507,   703,   466,   345,   588,   262,  6940,  2344,    13,
#         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,
#         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,
#         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,
#         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,
#         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,
#         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,
#         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,
#         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,
#         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,
#         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,
#         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]), tensor(0))
# data[1] =  (tensor([   51,   408,    88,   492,   489,    82,   466,   502,   257,  2661,
#            13,  1345,    82, 13878,   616, 10955, 12802,   284, 27168,  3972,
#           492,   489,    82,   288,   429,  6044,   340,    13,  6288,   318,
#           607, 10955,   911,  2926,   292, 50256, 50256, 50256, 50256, 50256,
#         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,
#         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,
#         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,
#         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,
#         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,
#         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,
#         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,
#         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]), tensor(0))
# data[2] =  (tensor([ 8413,  5540,    25,   921,   423,   407, 15680,   262,  2695,   345,
#           423,  1541,  3432,   329,    13,   402,  2069,  2638,  1378,  4598,
#           270,    13,   616,    76, 26730,    13, 31557,    14,   284,  2824,
#           534,  2695,    13, 50256, 50256, 50256, 50256, 50256, 50256, 50256,
#         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,
#         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,
#         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,
#         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,
#         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,
#         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,
#         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,
#         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]), tensor(1))

# 创建验证集
validate_dataset = DiySpamDataset(
    csv_file=Path("dataset") / "validation.csv",
    max_length=train_dataset.max_length,
    tokenizer=gpt2_tokenizer
)

# 创建测试集
test_dataset = DiySpamDataset(
    csv_file=Path("dataset") / "test.csv",
    max_length=train_dataset.max_length,
    tokenizer=gpt2_tokenizer
)



